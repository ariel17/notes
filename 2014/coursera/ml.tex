%        File: ml.tex
%     Created: Sat Mar 08 09:00 AM 2014 A
% Last Change: Sat Mar 08 09:00 AM 2014 A
%
\documentclass[a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{listings}
\everymath{\displaystyle}
\begin{document}

  \title{Machine Learning course on Coursera summarization}
  \author{Ariel Gerardo RÃ­os}
  \date{}
  \maketitle

  \tableofcontents

  \chapter{Begining}
    \section{Machine learning definition}
      \begin{itemize}
        \item Arthur Samuel (1959): Machine Learning: Field of study that gives
              computers the ability to learn without being explicitly
              programmed.
        \item Tom Mitchell (1998): Well-posed Learning Problem: A computer
              program is said to learn from experience E with respect to some
              task T and some performance measure P, if its performance on T,
              as measured by P, improves with experience E.
      \end{itemize}

    \section{Machine learning algorithms}
      \begin{itemize}
        \item To be seen on this course:
              \begin{itemize}
                \item Supervised learning
                \item Unsupervised learning
              \end{itemize}
        \item Others:
              \begin{itemize}
                \item Reinforcement learning
                \item Recommender systems
              \end{itemize}
      \end{itemize}
      Also talk about: Practical advice for applying learning algorithms.

  \section {Supervised learning}

    It assumes that the values given to learn are the right answers to the
    given question.

    \subsection{Regression problems}

      It tries to predict continuous valued output. Example: predict a house
      sell price.
      
    \subsection{Classification problems}

      It tries to assign a discrete value (classification) for a given output.
      Example: Cancer $\rightarrow$ \texttt{benign=0, malign=1}.


  \chapter{Linear regression with one variable}

    \section{Model representation}

      Notation:

      \textbf{m} = Number of training examples

      \textbf{x} = Input variables / features

      \textbf{y} = Output variables / target variable

      Example: Training set of housing prices

      \begin{center}
        \begin{tabular}{ | l | l |}
          \hline
          Size in feet$^2$ ($x$) & Price in \$1000's ($y$) \\ \hline
          2104 & 460 \\ \hline
          1416 & 232 \\ \hline
          1534 & 315 \\ \hline
          852  & 178 \\ \hline
          \ldots & \ldots \\ \hline
        \end{tabular}
      \end{center}

      \framebox[1.1\width][c]{Training set} \par
      \framebox[1.1\width]{Learning algorithm} \par

      Size of house $\rightarrow$ \framebox[1cm]{h} $\rightarrow$ Estimated
      price

  \chapter{Linear algebra review}

    \section{Matrices and vectors}

      \subsection{Matrix}

        \textbf{Definition:} Rectangular array of numbers.

        $A =
        \begin{bmatrix}
          1 & 2 & 3 \\
          4 & 5 & 6 \\
        \end{bmatrix}$

        \textbf{Dimension of matrix:} Number of rows $\times$ number of columns.
        
        This example has \textbf{2 rows} and \textbf{3 columns} and it contains
        Real numbers, so it can be represented as $\mathbb{R}^{2\times3}$.

      \subsubsection{Matrix elements}

        $A_{ij}$ = $i,j$ entry; in the $i$ row, $j$ column.

        Examples:

        $A_{11} = 1$
        
        $A_{23} = 6$

      \subsection{Vector}

        \textbf{Definition:} An $n \times 1$ matrix.

        $y =
        \begin{bmatrix}
          460 \\
          232 \\
          315 \\
          178 \\
        \end{bmatrix}$

        A 4-dimensional vector; $\mathbb{R}^4$

        \subsubsection{Vector elements}

          $y_{i}$ = $i$ element

          Example:

          $y_{1}$ = 460

          $y_{3}$ = 315


    \section{Conventions for the course}
          
        \begin{itemize}
          \item Naming: Uppercase letters for matrix naming and lowercase letter for vectors.
          \item 1-indexed vs 0-indexed: The course will use \textbf{1-indexed} vectors unless other condition is indicated.
        \end{itemize}


    \section{Addition and scalar multiplication}

      \subsection{Matrix addition}

        $\begin{bmatrix}
          1 && 0 \\
          2 && 5 \\
          3 && 1 \\
        \end{bmatrix}
        + 
        \begin{bmatrix}
          4 && 0.5 \\
          2 && 5 \\
          0 && 1 \\
        \end{bmatrix}
        =
        \begin{bmatrix}
          5 && 0.5 \\
          4 && 10 \\
          2 && 3 \\
        \end{bmatrix}$

        Both matrix participating on addition matchs on dimension and the result is another matrix with the same dimention.

        $\mathbb{R}^{3\times2} + \mathbb{R}^{3\times2} = \mathbb{R}^{3\times2}$

        Addition between matrix of different dimension cannot be done:

        $\mathbb{R}^{3\times2} + \mathbb{R}^{2\times2} =$ \textbf{Error}

      \subsection{Scalar multiplication}

        Scalar = Real number.

        $3 \times \begin{bmatrix}
          1 && 0 \\
          2 && 5 \\
          3 && 1 \\
        \end{bmatrix}
        =
        \begin{bmatrix}
          3 && 0 \\
          6 && 15 \\
          9 && 3 \\
        \end{bmatrix}$
        
        $\begin{bmatrix}
          4 && 0 \\
          6 && 3 \\
        \end{bmatrix}
        \div 4
        =
        \begin{bmatrix}
          1 && 0 \\
          3/2 && 3/4 \\
        \end{bmatrix}$

        $\mathbb{R} \times \mathbb{R}^{n \times m} = \mathbb{R}^{n \times m}$

      \subsection{Matrix-vector multiplication}

        $\begin{bmatrix}
          1 && 3 \\
          4 && 0 \\
          2 && 1 \\
        \end{bmatrix}
        \begin{bmatrix}
          1 \\
          5 \\
        \end{bmatrix}
        =
        \begin{bmatrix}
          16 \\
          4 \\
          7 \\
        \end{bmatrix}
        = r$

        $r_{11} = 1 \times 1 + 3 \times 5 = 16$
        
        $r_{21} = 4 \times 1 + 0 \times 5 = 4$

        $r_{31} = 2 \times 1 + 1 \times 5 = 7$

        The multiplication of matrix with different dimensions produces another matrix with a mix of them:

        $\mathbb{R}^{\textbf{3} \times 2} \times \mathbb{R}^{2 \times \textbf{1}} = \mathbb{R}^{\textbf{3} \times \textbf{1}}$

        Generalizing:

        $\mathbb{R}^{\textbf{a} \times b} \times \mathbb{R}^{c \times \textbf{d}} = \mathbb{R}^{\textbf{a} \times \textbf{d}}$

        Matrix $\times$ vector = vector

        \subsubsection{Performing a function as a matrix multiplication}

          Function:

          $h_{\theta}(x) = -40 + 0.25x$

          Function domain:

          $D_{h_{\theta}} = {2104, 1416, 1534, 852}$

          As a matrix multiplication:

          $\begin{bmatrix}
            1 & 2104 \\
            1 & 1416 \\
            1 & 1534
            1 & 852 \\
          \end{bmatrix}
          \times
          \begin{bmatrix}
            -40 \\
            0.25 \\
          \end{bmatrix}
          =
          \begin{bmatrix}
            -40 \times 1 + 0.25 \times 2104 \\
            -40 \times 1 + 0.25 \times 1416 \\
            -40 \times 1 + 0.25 \times 1534 \\
            -40 \times 1 + 0.25 \times 852 \\
          \end{bmatrix}$

          This is computationally more efficient to resolve this kind of
          problems this way:

          \begin{lstlisting}
            prediction = DataMatrix * parameters
          \end{lstlisting}

          than this way:

          \begin{lstlisting}
            for i:=1, 1000, \ldots,
                prediction = \dots
          \end{lstlisting}

      \subsection{Matrix-matrix multiplication}

        $\begin{bmatrix}
          1 & 3 & 2 \\
          4 & 0 & 1 \\
        \end{bmatrix}
        \begin{bmatrix}
          1 & 3 \\
          0 & 1 \\
          5 & 2 \\
        \end{bmatrix}
        =
        \begin{bmatrix}
            11 & 10 \\
            9 & 14 \\
        \end{bmatrix}$

        $\mathbb{R}^{2 \times 3} \times \mathbb{R}^{3 \times 2} = \mathbb{R}^{2 \times 2}$

        $\begin{bmatrix}
          1 & 3 & 2 \\
          4 & 0 & 1 \\
        \end{bmatrix}
        \begin{bmatrix}
          1 \\
          0 \\
          5 \\
        \end{bmatrix}
        =
        \begin{bmatrix}
          11 \\
          9 \\
        \end{bmatrix}$

        $\begin{bmatrix}
          1 & 3 & 2 \\
          4 & 0 & 1 \\
        \end{bmatrix}
        \begin{bmatrix}
          3 \\
          1 \\
          2 \\
        \end{bmatrix}
        =
        \begin{bmatrix}
          10 \\
          14 \\
        \end{bmatrix}$

        $A \times B = C$

        \begin{itemize}
          \item $A$ is a $m \times n$ matrix.
          \item $B$ is a $n \times o$ matrix.
          \item $C$ is a $m \times o$ matrix        \end{itemize}

        To be able to multiply, the number $n$ or rows on $B$ matrix must match the number of columns $n$ on $A$ matrix.

        $C_{i} = A \times B_{i}$

        \subsubsection{Performing multiple functions as a matrix multiplication}

          Functions:

          \begin{enumerate}
            \item $h_{\theta}(x) = -40 + 0.25x$
            \item $h_{\theta}(x) = 200 + 0.1x$
            \item $h_{\theta}(x) = -150 + 0.4x$
          \end{enumerate}

          Function domain:

          $D_{h_{\theta}} = {2104, 1416, 1534, 852}$

          As a matrix multiplication:

          $\begin{bmatrix}
            1 & 2104 \\
            1 & 1416 \\
            1 & 1534
            1 & 852 \\
          \end{bmatrix}
          \times
          \begin{bmatrix}
            -40 & 200 & -150 \\
            0.25 & 0.1 & 0.4 \\
          \end{bmatrix}
          =
          \begin{bmatrix}
            486 & 410 & 692 \\
            314 & 342 & 416 \\
            344 & 353 & 464 \\
            173 & 285 & 191 \\
          \end{bmatrix}$


      \subsection{Matrix multiplication properties}

        \begin{itemize}
          \item \textbf{Not conmutative:} $A$, $B$; matrices. In general, $A \times B \neq B \times A$.
          \item \textbf{Associative:} $A \times (B \times C) = (A \times B) \times C$
          \item \textbf{Identity matrix:} Denoted by $I$ or $I_{n \times n}$. It has $1$ in the diagonal and $0$ on any other position. Example of a $I_{3 \times 3}$:

            $\begin{bmatrix}
              1 & 0 & 1 \\
              0 & 1 & 0 \\
              0 & 0 & 1 \\
            \end{bmatrix}$

            For any matrix $A$: $A \times I = I \times A = A$

          \item \textbf{:} Denoted by $I$ or $I_{n \times n}$. It has $1$ in the diagonal and $0$ on any other position. Example of a $I_{3 \times 3}$:

        \end{itemize}

    \section{Inverse and transpose}

      \subsection{Inverse}

        $1$ = Identity

        Given a number, multiply it to another one to obtain the identity:

        $3 \times (3^{-1}) = 3 \times \frac{1}{3} = 1$

        Not all numbers have an inverse: $0^{0} = undefined$

        \subsubsection{Matrix inverse}

          If A is a $m \times m$ matrix (square matrix), and if it has an inverse:

          $A(A^{-1}) = A^{-1}A = I$

          \begin{itemize}
            \item Only square matrix can have an inverse.
            \item Matrices that don't have an inverse are some kind too close to zero.
            \item Matrices that don't have an inverse are "singular" or "degenerate".
          \end{itemize}

      \subsection{Matrix transpose}

        $A = 
        \begin{bmatrix}
          1 & 2 & 0 \\
          3 & 5 & 9 \\
        \end{bmatrix}
        \Rightarrow
        A^{T} = 
        \begin{bmatrix}
          1 & 3 \\
          2 & 5 \\
          0 & 9 \\
        \end{bmatrix}$

        Let $A$ be an $m \times n$ matrix, and let $B = A^{T}$. Then $B$ is an $n \times m$ matrix and $B_{ij} = A_{ji}$.

        Example:

        $B_{12} = A_{21} = 2$

  \chapter{Linear regression with multiple variables}

    \section{Multiple features}

      \begin{center}
        \begin{tabular}{ | l | l | l | l | l | }
          \hline
          Size in feet$^2$ ($x$) & Number of bedrooms & Number of floors & Age of home (years) & Price in \$1000's ($y$) \\ \hline
          2104 & 5 & 1 & 45 & 460 \\ \hline
          1416 & 3 & 2 & 40 & 232 \\ \hline
          1534 & 3 & 2 & 30 & 315 \\ \hline
          852  & 2 & 1 & 36 & 178 \\ \hline
          \ldots & \ldots & \dots & \dots & \ldots \\ \hline
        \end{tabular}
      \end{center}

      \textbf{Notation}:
      \begin{itemize}
        \item $n$ = number of features
        \item $x^{(i)}$ = input (features) of $i^{th}$ training example.
        \item $x^{(i)}_{j}$ = value of feature $j$ in $i^{th}$ training example.
      \end{itemize}

      $n = 4$
      $m = 47$

      $x^{(2)} = 
      \begin{bmatrix}
        1416 \\
        3 \\
        2 \\
        40 \\
      \end{bmatrix}$

      $x^{(2)}_3 = 2$

      \subsubsection{Hypothesis}

        $h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \ldots + \theta_{n}x_{n}$

        For convenience of notation, define $x_{0} = 1$.

        $x =
        \begin{bmatrix}
          x_{0} \\
          x_{1} \\
          x_{2} \\
          \vdots \\
          x_{n} \\
        \end{bmatrix}
        \in \mathbb{R}^{n+1}
        \ \ \ \ \ 
        \theta = 
        \begin{bmatrix}
          \theta_{0} \\
          \theta_{1} \\
          \theta_{2} \\
          \vdots \\
          \theta_{n} \\
        \end{bmatrix}
        \in \mathbb{R}^{n+1}$

        $h_{\theta}(x) = \theta_{0}x_{0} + \theta_{1}x_{1} + \ldots + \theta_{n}x_{n} = \theta^{T}x =
        \begin{bmatrix}
          \theta_{0} & \theta_{1} & \theta_{2} & \vdots & \theta_{n}
        \end{bmatrix}
        \begin{bmatrix}
          x_{0} \\
          x_{1} \\
          x_{2} \\
          \vdots \\
          x_{n} \\
        \end{bmatrix}$

        Also named \textbf{Multivariate linear regression}.

    \section{Gradient descent for multiple variables}

      \begin{itemize}
        \item Hypothesis: $h_{\theta}(x) = \theta^{T}x = \theta_{0}x_{0} + \theta_{1}x_{1} + \ldots + \theta_{n}x_{n}$
        \item Parameters: $\theta_{0}, \theta_{1}, \ldots, \theta_{n}$
        \item Cost function: $J(\theta_{0}, \theta_{1}, \ldots, \theta_{n}) = J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_{0}(x^{(i)}) - y^{(i)})^{2}$
        \item Gradient descent:

            Repeat \{

                \hspace{1cm} $\theta_{j} := \theta_{j} - \alpha\frac{\partial}{\partial\theta_{j}}J(\theta_{0}, \ldots, \theta_{n}) = \theta_{j} - \alpha\frac{\partial}{\partial\theta_{j}}J(\theta) = \theta_{j} - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})) - y^{(i)})x^{(i)}_{j}$

            \} \ \ (simultaneously update for every $j = 0, \ldots, n$)
      \end{itemize}


\end{document}
